{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Копия блокнота \"Тюмень_BERT_языковая модель.ipynb\"",
      "provenance": [],
      "collapsed_sections": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Shtepser/Manyinterpret/blob/master/Day2/%D0%A2%D1%8E%D0%BC%D0%B5%D0%BD%D1%8C_BERT_%D1%8F%D0%B7%D1%8B%D0%BA%D0%BE%D0%B2%D0%B0%D1%8F_%D0%BC%D0%BE%D0%B4%D0%B5%D0%BB%D1%8C_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1YeHbFB4oZS"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kcUk4jtJ5Z2f"
      },
      "source": [
        "В современной компьютерной лингвистике вычисление вероятности текста производится в основном за счёт нейронных, а не энграммных моделей. Существует много разновидностей архитектур, мы языковую модель с пропусками (MLM) `BERT`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vzXLAyUsNj8F"
      },
      "source": [
        "## Моделирование с пропусками: BERT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPuQVWP8NwjY"
      },
      "source": [
        "from transformers import AutoTokenizer, BertForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"bert-base-cased\").to(\"cuda\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgpXG7HVTHsB"
      },
      "source": [
        "В модели БЕРТ часть слов можно заменять масками. Модель предсказывает распределение вероятности для каждой позиции (в том числе для слов, присутствующих в тексте)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bzzlfalhTW_b"
      },
      "source": [
        " sentences = [\n",
        "     \"Yesterday, all my troubles seemed so far away.\",\n",
        "     \"Yesterday, all my [MASK] seemed so far away.\",\n",
        "     \"Several space rockets work on dymetylhydrasin.\",\n",
        "     \"The Starship prototype descended under active aerodynamic control, accomplished by four vehicles.\",\n",
        "     \"Спасибо, кончено — прощай, Москва!\"\n",
        " ]\n",
        " tokenization = tokenizer(sentences)\n",
        " for sentence in tokenization[\"input_ids\"]:\n",
        "     print(*sentence)\n",
        "     print(tokenizer.convert_ids_to_tokens(sentence))\n",
        "     print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.special_tokens_map"
      ],
      "metadata": {
        "id": "QS2wfESYkI6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vo85TgjjUNw9"
      },
      "source": [
        "Напишем функцию, которая находит самые вероятные слова на месте маски."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh2NTZi7T6OH"
      },
      "source": [
        "import torch\n",
        "\n",
        "def probable_words(sentence, tokenizer, model, k=10):\n",
        "    if sentence.count(\"[MASK]\") != 1:\n",
        "        raise ValueError(\"Маска должна быть ровно одна.\")\n",
        "    tokenization = tokenizer(sentence)[\"input_ids\"]\n",
        "    index = tokenization.index(tokenizer.mask_token_id)\n",
        "    # tensor: 1 * L\n",
        "    tensor = torch.LongTensor([tokenization]).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        # logits: L * K\n",
        "        logits = model(tensor)[\"logits\"][0]\n",
        "    probs = torch.softmax(logits, dim=-1)\n",
        "    # top_probs: k\n",
        "    top_probs, top_indexes = torch.topk(probs[index], dim=-1, k=k)\n",
        "    return tokenizer.convert_ids_to_tokens(top_indexes), top_probs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jLy4k33nWArX"
      },
      "source": [
        "sentence = \"The Starship prototype descended under active aerodynamic [MASK], accomplished by four vehicles.\"\n",
        "top_tokens, top_probs = probable_words(sentence, tokenizer, model)\n",
        "for token, prob in zip(top_tokens, top_probs):\n",
        "    print(f\"{token}:{prob:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04B5Kj_NKNZx"
      },
      "source": [
        "sentence = \"I liked that [MASK] cake.\"\n",
        "top_tokens, top_probs = probable_words(sentence, tokenizer, model)\n",
        "for token, prob in zip(top_tokens, top_probs):\n",
        "    print(f\"{token}:{prob:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Yesterday, all my [MASK] seemed so far away.\"\n",
        "top_tokens, top_probs = probable_words(sentence, tokenizer, model, k=20)\n",
        "for token, prob in zip(top_tokens, top_probs):\n",
        "    print(f\"{token}:{prob:.3f}\")"
      ],
      "metadata": {
        "id": "kAH4R4UuqeM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMDfx7UCYpBU"
      },
      "source": [
        "BERT также токенизует на сабтокены, а не слова."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlZGvFbFXrch"
      },
      "source": [
        "sentence = \"The Starship prototype descended under active aerodynamic control, accomplished by four vehicles.\"\n",
        "token_indexes = tokenizer(sentence)[\"input_ids\"]\n",
        "tokens = tokenizer.convert_ids_to_tokens(token_indexes)\n",
        "for index, token in zip(token_indexes, tokens):\n",
        "    print(index, token)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8kq2FFkYx_u"
      },
      "source": [
        "Соответственно, чтобы найти вероятность слова aerodynamic, нужно перемножить вероятности трёх сабтокенов разбиения."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1i0lIxzYxOB"
      },
      "source": [
        "def find_word_probability(sentence, word, tokenizer, model):\n",
        "    \"\"\"\n",
        "    sentence -- предложение, содержащее ровно один символ \"_\", обозначающий пропуск.\n",
        "    \"\"\"\n",
        "    if sentence.count(\"_\") != 1:\n",
        "        raise ValueError(\"Предложение должно содержать ровно один пропуск.\")\n",
        "    masked_sentence = sentence.replace(\"_\", tokenizer.mask_token)\n",
        "    masked_tokenization = tokenizer(masked_sentence)[\"input_ids\"]\n",
        "    word_tokenization = tokenizer(word, add_special_tokens=False)[\"input_ids\"]\n",
        "    word_length = len(word_tokenization)\n",
        "    index = masked_tokenization.index(tokenizer.mask_token_id)\n",
        "    masked_tokenization[index:index+1] = [tokenizer.mask_token_id] * word_length\n",
        "    batch = torch.LongTensor([masked_tokenization]).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(batch)[\"logits\"][0]\n",
        "    log_probs = torch.log_softmax(logits[index:index+word_length], dim=-1).cpu().numpy()\n",
        "    subtoken_log_probs = log_probs[np.arange(word_length), word_tokenization]\n",
        "    total_prob = subtoken_log_probs.sum()\n",
        "    return {\"total_log_prob\": total_prob, \"subtoken_probs\": np.exp(subtoken_log_probs)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UQZK3ogBSJT6"
      },
      "source": [
        "import numpy as np\n",
        "np.set_printoptions(precision=3)\n",
        "\n",
        "texts = [\n",
        "    \"Yesterday, all my _ seemed so far away.\", \n",
        "    \"The Starship prototype descended under active _ control, accomplished by four vehicles.\"\n",
        "] \n",
        "words = [\"troubles\", \"aerodynamic\"]\n",
        "for text, word in zip(texts, words):\n",
        "    print(text, word)\n",
        "    print(find_word_probability(text, word, tokenizer, model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yU6VtQhJUdYs"
      },
      "source": [
        "На самом деле BERT неадекватно вычисляет вероятности, когда присутствует несколько масок подряд. Поэтому маскировать сабтокены следует слева направо, чтобы при вычислении вероятности последнего сабтокена все остальные были уже заполнены.\n",
        "\n",
        "Это более точно соответствует формуле условной вероятности:\n",
        "$$\n",
        "p(t_1 ... t_k | \\mathrm{context}) = p(t_1 | \\mathrm{context}) p(t_2 | \\mathrm{context}, t_1) ... p(t_k | \\mathrm{context}, t_1, ..., t_{k-1})\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eyZByZuMRh-f"
      },
      "source": [
        "def find_word_probability(sentence, word, tokenizer, model):\n",
        "    \"\"\"\n",
        "    sentence -- предложение, содержащее ровно один символ \"_\", обозначающий пропуск.\n",
        "    \"\"\"\n",
        "    if sentence.count(\"_\") != 1:\n",
        "        raise ValueError(\"Предложение должно содержать ровно один пропуск.\")\n",
        "    masked_sentence = sentence.replace(\"_\", tokenizer.mask_token)\n",
        "    masked_tokenization = tokenizer(masked_sentence)[\"input_ids\"]\n",
        "    word_tokenization = tokenizer(word, add_special_tokens=False)[\"input_ids\"]\n",
        "    word_length = len(word_tokenization)\n",
        "    index = masked_tokenization.index(tokenizer.mask_token_id)\n",
        "    masked_tokenization[index:index+1] = [tokenizer.mask_token_id] * word_length\n",
        "    # повторяем маскированную токенизацию `word_length` раз\n",
        "    # [[active MASK MASK MASK control] [active MASK MASK MASK control] [active MASK MASK MASK control]]\n",
        "    batch = np.array([masked_tokenization] * word_length, dtype=int)\n",
        "    # заполняем начальные сабтокены\n",
        "    for prefix_length in range(1, word_length):\n",
        "        '''[\n",
        "            [active MASK(a) MASK MASK control] \n",
        "            [active a MASK(ero) MASK control] \n",
        "            [active a ero MASK(dynamic) control]\n",
        "        ]'''\n",
        "        batch[prefix_length, index:index+prefix_length] = word_tokenization[:prefix_length]\n",
        "    batch = torch.LongTensor(batch).to(\"cuda\")\n",
        "    with torch.no_grad():\n",
        "        logits = model(batch)[\"logits\"]\n",
        "    # в первом примере нам нужна позиция с номером index, далее index+1, ...\n",
        "    print(logits.shape)\n",
        "    log_probs = torch.log_softmax(\n",
        "        logits[np.arange(word_length),index+np.arange(word_length)], dim=-1\n",
        "    ).cpu().numpy()\n",
        "    print(log_probs.shape)\n",
        "    subtoken_log_probs = log_probs[np.arange(word_length), word_tokenization]\n",
        "    total_prob = subtoken_log_probs.sum()\n",
        "    return {\"total_log_prob\": total_prob, \"subtoken_probs\": np.exp(subtoken_log_probs)}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcPflGu6TC3G"
      },
      "source": [
        "np.set_printoptions(precision=3)\n",
        "\n",
        "texts = [\n",
        "    \"Yesterday, all my _ seemed so far away.\", \n",
        "    \"The Starship prototype descended under active _ control, accomplished by four vehicles.\"\n",
        "] \n",
        "words = [\"troubles\", \"aerodynamic\"]\n",
        "for text, word in zip(texts, words):\n",
        "    print(text, word)\n",
        "    print(find_word_probability(text, word, tokenizer, model))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YdZB53qOXmI5"
      },
      "source": [
        "В целом вероятностям БЕРТа не стоит слишком доверять, когда они вычисляются для слов, состоящих из нескольких подтокенов."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Модель для русского языка"
      ],
      "metadata": {
        "id": "rOoBNojKztat"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, BertForMaskedLM\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"DeepPavlov/rubert-base-cased\")\n",
        "model = BertForMaskedLM.from_pretrained(\"DeepPavlov/rubert-base-cased\").to(\"cuda\")"
      ],
      "metadata": {
        "id": "tujJvDw-zzDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Все счастливые семьи похожи друг на друга, каждая несчастная семья несчастлива по-своему.\"\n",
        "token_indexes = tokenizer(sentence)[\"input_ids\"]\n",
        "tokens = tokenizer.convert_ids_to_tokens(token_indexes)\n",
        "for index, token in zip(token_indexes, tokens):\n",
        "    print(index, token)"
      ],
      "metadata": {
        "id": "3Ad_uSuT0JKL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Что такое [MASK], это небо, плачущее небо под ногами.\"\n",
        "top_tokens, top_probs = probable_words(sentence, tokenizer, model)\n",
        "for token, prob in zip(top_tokens, top_probs):\n",
        "    print(f\"{token}:{prob:.3f}\")"
      ],
      "metadata": {
        "id": "jfZb3WSNz3gE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Проверим, как модель знает грамматику и другие аспекты языка."
      ],
      "metadata": {
        "id": "VBxbzX-WFBzo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Казань — город-[MASK], самый красивый город России.\"\n",
        "top_tokens, top_probs = probable_words(sentence, tokenizer, model)\n",
        "for token, prob in zip(top_tokens, top_probs):\n",
        "    print(f\"{token}:{prob:.3f}\")"
      ],
      "metadata": {
        "id": "-o__a4TgI8Fx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Съеденное вчера пирожное показалось ему очень [MASK] .\"\n",
        "top_tokens, top_probs = probable_words(sentence, tokenizer, model)\n",
        "for token, prob in zip(top_tokens, top_probs):\n",
        "    print(f\"{token}:{prob:.3f}\")"
      ],
      "metadata": {
        "id": "Ixm2Og64z3o2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Очень [MASK] показалось ему съеденное вчера пирожное.\"\n",
        "top_tokens, top_probs = probable_words(sentence, tokenizer, model)\n",
        "for token, prob in zip(top_tokens, top_probs):\n",
        "    print(f\"{token}:{prob:.3f}\")"
      ],
      "metadata": {
        "id": "bfERetn1JogB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Очень [MASK] пирожное.\"\n",
        "top_tokens, top_probs = probable_words(sentence, tokenizer, model)\n",
        "for token, prob in zip(top_tokens, top_probs):\n",
        "    print(f\"{token}:{prob:.3f}\")"
      ],
      "metadata": {
        "id": "bWVkXThpJxHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZhLFaxhNnEHm"
      },
      "source": [
        "sentence = \"Съеденная булка показалась ему очень [MASK], так что он купил ещё одну.\"\n",
        "top_tokens, top_probs = probable_words(sentence, tokenizer, model)\n",
        "for token, prob in zip(top_tokens, top_probs):\n",
        "    print(f\"{token}:{prob:.3f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Съеденная булка показалась ему очень [MASK], так что он потребовал деньги назад.\"\n",
        "top_tokens, top_probs = probable_words(sentence, tokenizer, model)\n",
        "for token, prob in zip(top_tokens, top_probs):\n",
        "    print(f\"{token}:{prob:.3f}\")"
      ],
      "metadata": {
        "id": "YLUyR3DpF_vZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Как мы видим, модель хорошо справляется с согласованием по роду, но куда хуже с пониманием тональности и смысла текста."
      ],
      "metadata": {
        "id": "boCV95iqFus6"
      }
    }
  ]
}